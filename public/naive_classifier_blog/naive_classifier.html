<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Naive Bayes Classifier for Essay Classification</title>
    <style>
      body {
        font-family: "Arial", sans-serif;
        background-color: #fff;
        margin: 0;
        padding: 0;
      }

      section {
        max-width: 800px;
        margin: 100px auto;
        padding: 20px;
        background-color: #fff;
        box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        border-radius: 10px;
      }

      h1,
      h2,
      h3 {
        color: #333;
      }

      p {
        color: #555;
      }

      img {
        max-width: 100%;
        height: auto;
        margin: 20px 0;
      }
    </style>
  </head>
  <body>
    <section>
      <h1>Building a Naive Bayes Classifier for Essay Classification</h1>

      <p>
        In this blog post, we delve into the process of building a Naive Bayes
        classifier for essay classification. The task involves distinguishing
        between essays generated by humans and those generated by an AI language
        model.
      </p>

      <h3>Methodology</h3>
      <p>
        The dataset initially lacked a sufficient number of AI-generated essays,
        posing a challenge. To address this, data augmentation was performed by
        generating medium-sized essays (100 words) using the Lama LLM model from
        Hugging Face in a Colab environment. Due to the memory limitations of
        Colab, an upgrade to Colab Pro (32 GB RAM) was necessary for generating
        additional essays using prompts from existing training samples.
      </p>
      <img
        src="./image-1.png"
        alt="connecting to huggingfacw and download llama llm"
      />
      <img src="./image-2.png" alt="testing the model with dummy prompt" />
      <h3>Model Construction</h3>
      <p>
        The Naive Bayes classifier was trained using a stratified k-fold
        approach, with features extracted through tokenization and stemming. The
        model leverages Laplace smoothing for better generalization and predicts
        whether an essay is human-generated or AI-generated based on word
        occurrences.
      </p>

      <h3>Reverse-Indexed Dictionary and Word Occurrence Lists</h3>
      <p>
        Central to the Naive Bayes classifier is the use of a reverse-indexed
        dictionary and word occurrence lists. These components play a crucial
        role in calculating the likelihood of an essay belonging to a particular
        class.
      </p>
      <img
        src="../image-3.png"
        alt="method to bulid reverse-indexed dict and word occurance list"
      />
      <p>
        <strong>Reverse-Indexed Dictionary:</strong> The reverse-indexed
        dictionary maps words to unique indices, facilitating efficient lookup
        during probability calculations. Each word in the dictionary is
        associated with a specific index, streamlining the process of retrieving
        relevant information during model training and prediction.
      </p>

      <p>
        <strong>Word Occurrence Lists:</strong> These lists maintain the
        occurrence count of each word in the dataset. For both human-generated
        and AI-generated essays, the frequency of words is recorded. During
        classification, these lists contribute to the probability calculation,
        allowing the model to discern the significance of each word in
        determining the class of an essay.
      </p>

      <p>
        The use of a reverse-indexed dictionary and word occurrence lists
        enhances the classifier's ability to generalize and make informed
        predictions. It provides a structured representation of the dataset,
        enabling efficient computation of class probabilities based on the
        presence of specific words in an essay.
      </p>

      <h3>Training and Evaluation</h3>
      <p>
        The training process involved experimenting with different folds and
        selecting the model with the highest accuracy. The final model
        parameters were saved for future predictions. The evaluation metric used
        was accuracy, measuring the classifier's ability to correctly predict
        the class of essays.
      </p>
      <img
        src="./image-4.png"
        alt="method to traain the classifier using kfold validation"
      />
      <h3>Conclusion</h3>
      <p>
        Building a Naive Bayes classifier for essay classification is a
        challenging yet rewarding task. The methodology, including data
        augmentation and model training, contributes to the effectiveness of the
        classifier in distinguishing between human and AI-generated essays.
      </p>

      <h2>Additional Points</h2>
      <ul>
        <li>
          The dataset augmentation process was essential due to the insufficient
          number of AI-generated essays in the initial dataset.
        </li>
        <li>
          Generating essays using the Lama LLM model in a Colab environment
          required careful management of RAM, leading to an upgrade to Colab Pro
          for increased memory.
        </li>
      </ul>
    </section>
  </body>
</html>
